---
phase: 00-baseline-audit
plan: 02
type: execute
wave: 2
depends_on:
  - 00-01
files_modified:
  - cloudbuild.yaml
autonomous: false
requirements:
  - BASE-01
  - BASE-02
  - BASE-03
  - BASE-04

must_haves:
  truths:
    - "The Cloud Run service is deployed with --timeout=3600 to support the ~25-60 minute audit run"
    - "The user has triggered POST /audit and received a 200 response with a report_url"
    - "The Markdown report exists in Firebase Storage and is accessible via the signed URL"
    - "The report contains a timing table with all 15 result rows (3 quants × 5 guidance values)"
    - "The report contains the torch.compile viability section with graph break count and recommendation"
    - "The user has reviewed all 15 output images and identified the best quantization and guidance scale settings for Phase 1"
    - "Approved settings are recorded in the report or STATE.md for Phase 1 to consume"
  artifacts:
    - path: "cloudbuild.yaml"
      provides: "Cloud Run deploy step updated with --timeout=3600"
      contains: "timeout=3600"
  key_links:
    - from: "cloudbuild.yaml"
      to: "Cloud Run service"
      via: "gcloud run deploy --timeout=3600"
      pattern: "timeout=3600"
---

<objective>
Update the Cloud Run deployment configuration to support the long-running audit, then pause for the user to deploy, trigger the audit, and record approved Phase 1 settings.

Purpose: The audit endpoint can take 25-60 minutes. The current Cloud Run timeout of 300 seconds will kill the request before the matrix completes. This plan updates the timeout and hands off to the user to run the audit.

Output:
- cloudbuild.yaml — --timeout=3600 added to Cloud Run deploy step
- Human review of audit report and approval of Phase 1 settings
</objective>

<execution_context>
@/Users/colinwatson/.claude/get-shit-done/workflows/execute-plan.md
@/Users/colinwatson/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@cloudbuild.yaml
@.planning/phases/00-baseline-audit/00-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update cloudbuild.yaml to extend Cloud Run request timeout to 3600s</name>
  <files>cloudbuild.yaml</files>
  <action>
In cloudbuild.yaml, find the `gcloud run deploy` step. The current deploy args include `"--timeout=300"`. Change this to `"--timeout=3600"`.

The Cloud Run `--timeout` flag sets the maximum duration for a single request. The audit endpoint is synchronous and can take 25-60 minutes (matrix of 15 inferences + 3 model loads + compile check). 3600 seconds (60 minutes) provides sufficient headroom.

No other changes to cloudbuild.yaml are needed. The existing GPU config (nvidia-l4), memory (32Gi), CPU (8), and other settings are correct for the audit.

After making the change, also add a comment above the deploy step (or inline) noting that 3600s is required for the baseline audit endpoint:
```yaml
# timeout=3600 required for /audit endpoint (full matrix ~25-60 min on L4)
```

Place the comment as a yaml comment on the line immediately before the `--timeout=3600` arg, or as a standalone comment before the deploy step — whichever fits the existing yaml structure.
  </action>
  <verify>
grep -n "timeout" cloudbuild.yaml
# Should show both the build-level timeout: 3600s AND the --timeout=3600 in the deploy args
# Confirm the --timeout value in the run deploy args changed from 300 to 3600
  </verify>
  <done>
cloudbuild.yaml has --timeout=3600 in the Cloud Run deploy step (changed from 300).
The build-level `timeout: 3600s` at the bottom of the file is unchanged.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Deploy audit build and run full parameter matrix on session D1A406F5</name>
  <what-built>
  The complete audit system:
  - POST /audit endpoint (app/routers/audit.py)
  - Parameter matrix runner with timing, VRAM cleanup, torch.compile check (app/services/audit_runner.py)
  - generate_signed_url() / upload_and_sign() helpers (app/services/firebase.py)
  - Cloud Run timeout extended to 3600s (cloudbuild.yaml)
  </what-built>
  <how-to-verify>
  **Step 1: Deploy the audit build**
  Trigger a Cloud Build run (via GitHub push to main, or manually via GCP Console: Cloud Build → Triggers → Run). Wait for the build to complete (~10-15 min including model bake). Confirm the Cloud Run service is updated with the new image.

  **Step 2: Trigger the audit endpoint**
  The Cloud Run service requires authentication. Use a service account token or gcloud:
  ```bash
  TOKEN=$(gcloud auth print-identity-token)
  SERVICE_URL=$(gcloud run services describe pawfect-edit-inpaint --region=us-east4 --format='value(status.url)')
  curl -X POST "${SERVICE_URL}/audit" \
    -H "Authorization: Bearer ${TOKEN}" \
    -H "Content-Type: application/json" \
    -d '{"user_id": "<USER_ID_FOR_D1A406F5>", "session_id": "D1A406F5"}' \
    --max-time 7200
  ```
  Replace `<USER_ID_FOR_D1A406F5>` with the actual user ID for that session.
  The request will run for 25-60 minutes. On success, the response body contains `report_url`.

  **Step 3: Review the audit report**
  Open the `report_url` from the response (it's a signed Firebase Storage URL, valid 72 hours).
  The report contains:
  - GPU identity (confirm it says NVIDIA L4, sm_89)
  - Results matrix table with 15 rows (Q4_0 × 5, Q5_K_S × 5, Q8_0 × 5)
  - Wall-clock and GPU timing per inference
  - Links to all 15 output images — open each and visually compare

  **Step 4: Identify best settings**
  Review all 15 output images side-by-side. Both hallucinated objects AND color/texture mismatch are failures. Subtle seams are acceptable.
  Identify:
  - Best quantization variant (look for: does Q8_0 quality noticeably beat Q4_0? is Q8_0 faster?)
  - Best guidance scale (look for: which value produces the most natural grass/leaf fill without hallucinated objects?)

  **Step 5: Record approved settings**
  Add your findings to .planning/STATE.md under "Decisions":
  ```
  - Phase 0 approved: quantization = [Q4_0|Q5_K_S|Q8_0], guidance_scale = [value], notes = [observations]
  - Phase 0 compile: torch.compile viable = [yes|no], break count = [N]
  ```

  **Expected outcomes (approximate — actual results may differ):**
  - Q8_0 should produce visibly sharper fill than Q4_0 (research: "noticeable quality degradation" in Q4_0)
  - Guidance scale 2-5 should produce more naturalistic background fills vs. 20-30 (community finding — must be verified)
  - L4 (sm_89) confirmed — torch.compile should show zero or very few graph breaks
  </how-to-verify>
  <action>
  Human action required: deploy the build via Cloud Build trigger (GitHub push to main or manual trigger in GCP Console), then call POST /audit with the D1A406F5 session credentials, wait for the response (~25-60 minutes), open the report_url, review all 15 output images, and record the best quantization and guidance scale settings in STATE.md.
  </action>
  <verify>
  grep -A5 "Phase 0 approved" .planning/STATE.md
  # Must show: quantization choice, guidance scale choice, torch.compile viable status
  </verify>
  <done>Approved settings recorded in STATE.md. Report confirmed: 15 result rows, GPU identity = NVIDIA L4 sm_89, torch.compile viability section present.</done>
  <resume-signal>Type "approved" after recording settings in STATE.md, or describe any issues encountered (e.g., timeout errors, VRAM OOM, permission errors on signed URLs).</resume-signal>
</task>

</tasks>

<verification>
After checkpoint approval:
1. grep -A5 "Phase 0 approved" .planning/STATE.md — confirm approved settings recorded
2. The report_url was accessible and showed 15 result rows
3. GPU identity in report confirmed L4 sm_89
4. torch.compile viability section present in report
</verification>

<success_criteria>
- cloudbuild.yaml has --timeout=3600 in the Cloud Run deploy step
- Audit deployed and ran successfully on L4 hardware
- All 15 output images are visible in the report
- User has reviewed images and recorded approved quantization and guidance scale settings in STATE.md
- torch.compile viability is confirmed or rejected based on actual dynamo.explain() output on L4
- Phase 0 blockers in STATE.md (quantization contribution to hallucinations, guidance scale target) are resolved with empirical data
</success_criteria>

<output>
After completion, create `.planning/phases/00-baseline-audit/00-02-SUMMARY.md`
</output>
